{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glow GWAS QC Tutorial \n",
    "\n",
    "This documenent contains a replication of the tutorial pipeline in [A tutorial on conducting genomeâ€wide association studies:\n",
    "Quality control and statistical analysis](https://www.ncbi.nlm.nih.gov/pubmed/29484742/) (Marees et al. 2017) using Spark and Glow.\n",
    "\n",
    "The data for this tutorial comes from a HapMap (phase III) and consists of 1,457,897 SNPs for 165 individuals. The documentation for the publication above states that:\n",
    "\n",
    "> This tutorial uses freely available HapMap data: hapmap3_r3_b36_fwd.consensus.qc. We simulated a binary outcome measure (i.e., a binary phenotypic trait) and added this to the dataset. The outcome measure was only simulated for the founders in the HapMap data. This data set will be referred to as HapMap_3_r3_1. The HapMap data, without our simulated outcome measure, can also be obtained from http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-05_phaseIII/plink_format/ \n",
    "\n",
    "The dataset itself is very small.  File size by format:\n",
    "\n",
    "- ```HapMap_3_r3_1.{bed, bim, fam}```: 98M \n",
    "- ```HapMap_3_r3_1.parquet```: 92M (snappy compressed)\n",
    "- ```HapMap_3_r3_1.vcf```: 968M (uncompressed)\n",
    "- ```HapMap_3_r3_1.mt```: 101M (hail MatrixTable)\n",
    "\n",
    "#### Companion Analysis\n",
    "\n",
    "See [gwas-tutorial-plink](gwas-tutorial-plink) for the ```plink``` commands used in the tutorial publication and their corresponding outputs.\n",
    "\n",
    "#### Contents\n",
    "\n",
    "- [Load Raw Data](#load_raw_data): Reading plink files (.bed, .bim, .fam)\n",
    "    - **Wide vs Long Format**: Comparing simple operations with vcf-esque wide format to stacked, long format\n",
    "    - **Phenotypes and Demographics**: Loading gender and outcome data (the outcome is simulated in this case but everything else is real)\n",
    "- [Step 1: Sample/Variant Absence Filter](#step_1)\n",
    "- [Step 2: Gender Discrepancy](#step_2)\n",
    "- [Step 3: Autosomal Variants and MAF Filtering](#step_3)\n",
    "- [Step 4: Hardy-Weinberg Equilibrium Filtering](#step_4)\n",
    "- [Step 5: Heterozygosity Filtering](#step_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:16:29.309303Z",
     "start_time": "2020-01-16T04:16:23.888Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/01/16 04:16:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.41.3.min',\n",
       "    jquery: 'https://code.jquery.com/jquery-3.3.1.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , paths._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$          , glow._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$               , benchmark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                             , init_plotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36msys.process._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.projectglow.Glow\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.Almond.{init => init_plotly_js, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbetter.files.File\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeop\u001b[39m\n",
       "\u001b[36mss\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@631e51e9\n",
       "\u001b[32mimport \u001b[39m\u001b[36mss.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mdata_dir\u001b[39m: \u001b[32mFile\u001b[39m = /home/eczech/data/gwas/tutorial/1_QC_GWAS"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load samtools lib before glow as this is necessary to avoid this error on vcf writes:\n",
    "// htsjdk.variant.variantcontext.VariantContextBuilder.getGenotypes() method not found\n",
    "import $ivy.`com.github.samtools:htsjdk:2.21.1`\n",
    "import $file.^.init.spark, spark._\n",
    "import $file.^.init.paths, paths._\n",
    "import $file.^.init.glow, glow._\n",
    "import $file.^.init.benchmark, benchmark._\n",
    "import $file.^.init.{plotly => init_plotly}, init_plotly._\n",
    "import $file.^.init.{functions => glow_functions}, glow_functions._\n",
    "import sys.process._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import io.projectglow.Glow\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond.{init => init_plotly_js, _}\n",
    "import better.files.File\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"io.projectglow.plink\").setLevel(Level.WARN)\n",
    "\n",
    "def timeop[T](op: String)(block: => T) = optimer(\"glow\", op, block)\n",
    "\n",
    "val ss = getLocalSparkSession()\n",
    "import ss.implicits._\n",
    "Glow.register(ss)\n",
    "\n",
    "init_plotly_js(offline=false)\n",
    "\n",
    "val data_dir = GWAS_TUTORIAL_DATA_DIR / \"1_QC_GWAS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:16:30.151166Z",
     "start_time": "2020-01-16T04:16:23.890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgetOrCreate\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Helper function for fetching materialized datasets\n",
    "def getOrCreate(path: File, fn: => DataFrame): DataFrame = {\n",
    "    if (!path.exists)\n",
    "        fn.write.parquet(path.toString)\n",
    "    ss.read.parquet(path.toString)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"load_raw_data\">Load Raw Data</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this tutorial is stored according to the ```plink``` specification discussed, in detail, in the [publication](https://www.ncbi.nlm.nih.gov/pubmed/29484742/) section entitled \"2.1 Data Format\".  Fortunately, there is a [plink](https://glow.readthedocs.io/en/latest/etl/variant-data.html#plink) input format provided within Glow that can be used to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:16:35.331997Z",
     "start_time": "2020-01-16T04:16:23.892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contigName: string (nullable = true)\n",
      " |-- names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- start: long (nullable = true)\n",
      " |-- end: long (nullable = true)\n",
      " |-- referenceAllele: string (nullable = true)\n",
      " |-- alternateAlleles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genotypes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- sampleId: string (nullable = true)\n",
      " |    |    |-- calls: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Immediately load and cache the plink dataset as parquet\n",
    "val df = getOrCreate(\n",
    "    File(data_dir / QC0_FILE + \".parquet\"),\n",
    "    ss.read.format(\"plink\").load(data_dir / QC0_FILE + \".bed\" toString)\n",
    ")\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:16:37.103746Z",
     "start_time": "2020-01-16T04:16:23.894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+---------+---------+---------------+----------------+\n",
      "|contigName|names       |position|start    |end      |referenceAllele|alternateAlleles|\n",
      "+----------+------------+--------+---------+---------+---------------+----------------+\n",
      "|6         |[rs1890312] |0.0     |153585564|153585565|G              |[A]             |\n",
      "|6         |[rs317105]  |0.0     |153585687|153585688|G              |[A]             |\n",
      "|6         |[rs10499272]|0.0     |153585724|153585725|G              |[C]             |\n",
      "+----------+------------+--------+---------+---------+---------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"genotypes\").show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:16:38.065107Z",
     "start_time": "2020-01-16T04:16:23.895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|variant  |sampleId    |calls |\n",
      "+---------+------------+------+\n",
      "|rs1890312|1328_NA06989|[0, 0]|\n",
      "|rs1890312|1377_NA11891|[0, 0]|\n",
      "|rs1890312|1349_NA11843|[0, 0]|\n",
      "+---------+------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "    .selectExpr(\"element_at(names, 1) as variant\", \"explode(genotypes) as genotypes\")\n",
    "    .select(\"variant\", \"genotypes.*\")\n",
    "    .show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see how often the ```names``` and ```alternateAlleles``` fields have more or less than one value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:16:39.264462Z",
     "start_time": "2020-01-16T04:16:23.897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------+\n",
      "|size(names)|size(alternateAlleles)|\n",
      "+-----------+----------------------+\n",
      "|          1|                     1|\n",
      "+-----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(size($\"names\"), size($\"alternateAlleles\")).dropDuplicates().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T02:50:48.394391Z",
     "start_time": "2020-01-16T02:50:48.298Z"
    }
   },
   "source": [
    "Show allele encoding frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:16:40.528659Z",
     "start_time": "2020-01-16T04:16:23.898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+------+\n",
      "|referenceAllele|alternateAlleles[0]| count|\n",
      "+---------------+-------------------+------+\n",
      "|              G|                  A|291730|\n",
      "|              G|                  C| 34388|\n",
      "|              C|                  T|292773|\n",
      "|              T|                  G| 60109|\n",
      "|              A|                  G|252469|\n",
      "|              C|                  G| 34331|\n",
      "|              C|                  A| 66008|\n",
      "|              T|                  C|252457|\n",
      "|              A|                  C| 60036|\n",
      "|              G|                  T| 65637|\n",
      "|              T|                  A| 23748|\n",
      "|              A|                  T| 23868|\n",
      "|              I|                  D|   226|\n",
      "|              D|                  I|   117|\n",
      "+---------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// The allele encodings include \"D\"/\"I\", which will be problematic later on\n",
    "df.groupBy($\"referenceAllele\", $\"alternateAlleles\"(0)).count.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T02:51:21.255237Z",
     "start_time": "2020-01-16T02:51:21.209Z"
    }
   },
   "source": [
    "Show that dataset is dense (i.e. ```genotypes``` array has constant size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:16:54.864215Z",
     "start_time": "2020-01-16T04:16:23.900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|size(genotypes)|  count|\n",
      "+---------------+-------+\n",
      "|            165|1457897|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(size($\"genotypes\")).count.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull a count of variants and samples/individuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:16:55.612051Z",
     "start_time": "2020-01-16T04:16:23.903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1457897,165)\n",
      "Elapsed time: 0.3 seconds\n"
     ]
    }
   ],
   "source": [
    "timeop(\"qc0-count\") {\n",
    "    println(count(df))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:15.011130Z",
     "start_time": "2020-01-16T04:16:23.905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1457897,165)\n",
      "Elapsed time: 19.2 seconds\n"
     ]
    }
   ],
   "source": [
    "timeop(\"qc0-count-unique\") {\n",
    "    println(uniqueCount(df))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide vs Long Format\n",
    "\n",
    "Note that counting unique samples and variants was actually quite slow.  This takes around 20 seconds in local mode yet a similar ```plink``` operation that fetches this information takes less than 1 second.  One possible reason for this could be that wide data formats (i.e. a large array in each row) are more difficult to optimize than long formats.  The code below will stack the raw plink dataset to produce a row for each variant and sample combination (rather than data for all samples being in each per-variant row) and see if a simple counting operation like this runs more quickly.\n",
    "\n",
    "**note**: the \"_lf\" (aka long format) suffix will be applied to any data frame in this document as a convention indicating the change in structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:15.164306Z",
     "start_time": "2020-01-16T04:16:23.907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdf_lf\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create the \"long format\" table by exploding the genotype calls into their own rows\n",
    "def df_lf = df.withColumn(\"genotypes\", explode($\"genotypes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:38.985620Z",
     "start_time": "2020-01-16T04:16:23.908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|numSamples|numVariants|\n",
      "+----------+-----------+\n",
      "|       165|    1457897|\n",
      "+----------+-----------+\n",
      "\n",
      "Elapsed time: 23.6 seconds\n"
     ]
    }
   ],
   "source": [
    "// Run the same counting operation\n",
    "timeop(\"qc0-count-unique-lf\") {\n",
    "    df_lf.agg(\n",
    "        countDistinct(\"genotypes.sampleId\").as(\"numSamples\"), \n",
    "        countDistinct($\"names\"(0)).as(\"numVariants\")\n",
    "    ).show\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting on top of a materialized dataset in long format was actually slower in this case (about ~21s instead of 17s).  The tutorial dataset only consists of ~1.5M SNPs and 165 people so it may be that the opposite becomes true for a sufficiently large experiment.  The plink command below shows how similar information can be retrieved for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:39.631904Z",
     "start_time": "2020-01-16T04:16:23.909Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 225 het. haploid genotypes present (see plink.hh ); many commands\n",
      "treat these as missing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.4 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres13\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"1457897 variants loaded from .bim file.\n",
       "165 people (80 males, 85 females) loaded from .fam.\n",
       "\"\"\"\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeop(\"qc0-count-plink\") {\n",
    "    (s\"/usr/local/plink/plink --bfile ${data_dir}/HapMap_3_r3_1 --freq counts\" #| \"grep people\\\\|variants\").!!\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T18:39:58.999756Z",
     "start_time": "2020-01-10T18:39:58.864Z"
    }
   },
   "source": [
    "### Phenotypes and Demographics\n",
    "\n",
    "The gender and phenotype information for this data is stored in a separate .fam file, which per the [plink specification](https://www.cog-genomics.org/plink/1.9/formats#fam) is a headerless, space-delimited csv file.\n",
    "\n",
    "A less obvious but key feature of this dataset includes the ability to identify \"founders\" in a GWAS population.  A population founder is defined generally as one of a small number of individuals isolated from a larger population that spawn a new but bottlenecked population.  This could be defined more specifically in a number of ways but in the context of ```plink```, this has a very specific definition (see [here](http://zzz.bwh.harvard.edu/plink/thresh.shtml#maf)) stating that founders are:\n",
    "\n",
    "> individuals for whom the paternal and maternal individual codes and both 0.\n",
    "\n",
    "The ```iidp``` and ```iidm``` codes are either \"0\" for no parent or equal to the ```iid``` of another individual in the dataset.  Founders are then identifiable where both of these values are \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:40.298608Z",
     "start_time": "2020-01-16T04:16:23.911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+----+------+---------+------------+\n",
      "| fid|    iid|iidp|iidm|gender|phenotype|    sampleId|\n",
      "+----+-------+----+----+------+---------+------------+\n",
      "|1328|NA06989|   0|   0|     2|        2|1328_NA06989|\n",
      "|1377|NA11891|   0|   0|     1|        2|1377_NA11891|\n",
      "|1349|NA11843|   0|   0|     1|        1|1349_NA11843|\n",
      "+----+-------+----+----+------+---------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdp\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dp = ss.read.option(\"delimiter\", \" \")\n",
    "    .csv(data_dir / QC0_FILE + \".fam\" toString)\n",
    "    .toDF(\"fid\",\"iid\",\"iidp\",\"iidm\", \"gender\", \"phenotype\")\n",
    "    .withColumn(\"sampleId\", concat($\"fid\", lit(\"_\"), $\"iid\"))\n",
    "dp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:40.852352Z",
     "start_time": "2020-01-16T04:16:23.912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+\n",
      "|gender|phenotype|count|\n",
      "+------+---------+-----+\n",
      "|     1|       -9|   23|\n",
      "|     1|        1|   27|\n",
      "|     1|        2|   30|\n",
      "|     2|       -9|   30|\n",
      "|     2|        1|   29|\n",
      "|     2|        2|   26|\n",
      "+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Note that phenotypes can be missing (-9) but gender is always present in this case\n",
    "dp.groupBy(\"gender\", \"phenotype\").count.sort($\"gender\", $\"phenotype\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:41.531005Z",
     "start_time": "2020-01-16T04:16:23.913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+----------+-----+\n",
      "|has_father_in_dataset|has_mother_in_dataset|is_founder|count|\n",
      "+---------------------+---------------------+----------+-----+\n",
      "|                false|                false|      true|  112|\n",
      "|                 true|                 true|     false|   48|\n",
      "|                false|                 true|     false|    3|\n",
      "|                 true|                false|     false|    2|\n",
      "+---------------------+---------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dp\n",
    "    .withColumn(\"has_father_in_dataset\", $\"iidp\" =!= \"0\")\n",
    "    .withColumn(\"has_mother_in_dataset\", $\"iidm\" =!= \"0\")\n",
    "    .withColumn(\"is_founder\", !$\"has_father_in_dataset\" && !$\"has_mother_in_dataset\")\n",
    "    .groupBy(\"has_father_in_dataset\", \"has_mother_in_dataset\", \"is_founder\")\n",
    "    .count.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"step_1\">Step 1: Sample/Variant Absence Filter</a></h2>\n",
    "\n",
    "This section covers the tutorial steps related to filtering variants and samples based on their missingness in the data (aka call rate).  These steps are defined as:\n",
    "\n",
    "- Delete SNPs with missingness >0.2.\n",
    "- Delete individuals with missingness >0.2.\n",
    "- Delete SNPs with missingness >0.02.\n",
    "- Delete individuals with missingness >0.02 \n",
    "  \n",
    "In Glow, [Sample Quality Control](https://glow.readthedocs.io/en/latest/etl/sample-qc.html) functions produce summary statistics that are useful for sample (i.e. individual) QC, including call rates.  The function ```sample_call_summary_stats``` is an aggregation that when applied to an entire data frame will produce a single row with a big array containing all the data for each sample.  See [SampleCallSummaryStats.scala](https://github.com/projectglow/glow/blob/75ea111e0ba86001ea213df103373125b8732778/core/src/main/scala/io/projectglow/sql/expressions/SampleCallSummaryStats.scala#L42) for implementation details.\n",
    "\n",
    "Similarly, [Variant Quality Control](https://glow.readthedocs.io/en/latest/etl/variant-qc.html) functions produce statistics across samples for individual variants.  The ```call_summary_stats``` function is a transformation that will produce statistics for each row (since the plink/vcf convention is to have one variant per row with all the samples in columns, this is possible as a per-row transformation). See [VariantQcExprs.scala](https://github.com/projectglow/glow/blob/master/core/src/main/scala/io/projectglow/sql/expressions/VariantQcExprs.scala#L168) for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:41.769128Z",
     "start_time": "2020-01-16T04:16:23.915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// In each step, *materialized* datasets will be loaded to base all operations on for that\n",
    "// step to ensure that execution time comparisons are compatible (otherwise they are\n",
    "// dependent on the performance of previous steps)\n",
    "val df = ss.read.parquet(data_dir / QC0_FILE + \".parquet\" toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Filtering\n",
    "\n",
    "Use the built-in glow function to get per-sample QC metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:41.921134Z",
     "start_time": "2020-01-16T04:16:23.917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- qc: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- sampleId: string (nullable = true)\n",
      " |    |    |-- callRate: double (nullable = true)\n",
      " |    |    |-- nCalled: long (nullable = true)\n",
      " |    |    |-- nUncalled: long (nullable = true)\n",
      " |    |    |-- nHomRef: long (nullable = true)\n",
      " |    |    |-- nHet: long (nullable = true)\n",
      " |    |    |-- nHomVar: long (nullable = true)\n",
      " |    |    |-- nSnp: long (nullable = true)\n",
      " |    |    |-- nInsertion: long (nullable = true)\n",
      " |    |    |-- nDeletion: long (nullable = true)\n",
      " |    |    |-- nTransition: long (nullable = true)\n",
      " |    |    |-- nTransversion: long (nullable = true)\n",
      " |    |    |-- nSpanningDeletion: long (nullable = true)\n",
      " |    |    |-- rTiTv: double (nullable = true)\n",
      " |    |    |-- rInsertionDeletion: double (nullable = true)\n",
      " |    |    |-- rHetHomVar: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdf_stat\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_stat = df.selectExpr(\"sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc\")\n",
    "df_stat.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that this is a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:42.161007Z",
     "start_time": "2020-01-16T04:16:23.918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres19\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1L\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stat.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that exploding this array gives a number of rows equal to expected number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:17:57.178472Z",
     "start_time": "2020-01-16T04:16:23.919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres20\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m165L\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stat.selectExpr(\"explode(qc)\").count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show what the call rate distribution looks like across all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:18:12.829579Z",
     "start_time": "2020-01-16T04:16:23.920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "histogram",
         "x": [
          0.9971170802875648,
          0.9857417910867503,
          0.99892722188193,
          0.9957349524692074,
          0.9797077571323626,
          0.9981953457617376,
          0.9933891077353201,
          0.9974017368853904,
          0.9963310165258589,
          0.9987941534964404,
          0.9987001825231824,
          0.9987735759110554,
          0.9989793517649052,
          0.9985328181620512,
          0.9982872589764572,
          0.9987090994768492,
          0.9984902911522556,
          0.9973235420609274,
          0.9985149842547176,
          0.997026538911871,
          0.9982070063934558,
          0.9984703994863835,
          0.9989450557892636,
          0.9989347669965711,
          0.9983496776521249,
          0.9866053637534065,
          0.9989697488917255,
          0.9974717006756993,
          0.9968365392068164,
          0.9956231475886157,
          0.9922120698512995,
          0.9958309812010039,
          0.9991858135382677,
          0.9936058583013752,
          0.9983778003521511,
          0.999229712387089,
          0.9981713385787885,
          0.9974168271146727,
          0.9989059583770321,
          0.9974085960805187,
          0.9963810886502956,
          0.9951436898491457,
          0.9932766169352156,
          0.996797441794585,
          0.9993476905432963,
          0.9977776207784226,
          0.9979024581297581,
          0.9988744060794418,
          0.999143972447985,
          0.9991035031967279,
          0.9989217345258272,
          0.9975416644660082,
          0.995216397317506,
          0.9973324590145943,
          0.9973592098755948,
          0.9946169036632904,
          0.9973242279804403,
          0.9988915540672626,
          0.9974970796976741,
          0.9979100032443993,
          0.9987001825231824,
          0.9959057464279026,
          0.9985053813815379,
          0.9979964291030162,
          0.9983325296643041,
          0.9986418793645916,
          0.9991871853772935,
          0.9991041891162408,
          0.9817325915342442,
          0.9969881274191524,
          0.9968084165067903,
          0.9978818805443732,
          0.9974874768244945,
          0.9993950189896817,
          0.9957850245936442,
          0.9988826371135958,
          0.9976644440588053,
          0.9991426006089593,
          0.9993620948530657,
          0.9995349465702995,
          0.9926160764443579,
          0.9986213017792066,
          0.9974312314244422,
          0.9978976566931683,
          0.9994793870897601,
          0.9995562100751973,
          0.9978468986492187,
          0.9987475109695678,
          0.9988016986110816,
          0.9978338661784749,
          0.9983263563886886,
          0.998595236837719,
          0.9993771850823481,
          0.9983277282277143,
          0.9943445936166958,
          0.9992804704310387,
          0.9946594306730859,
          0.9985170420132561,
          0.9991377991723696,
          0.9995802172581465,
          0.9993867879555277,
          0.999410109218964,
          0.9994780152507344,
          0.9991734669870368,
          0.9986919514890283,
          0.9988668609648007,
          0.9978578733614241,
          0.9915714210263139,
          0.9934398657792697,
          0.9991961023309602,
          0.9949879861197327,
          0.9975773322806755,
          0.9977542995149863,
          0.9992427448578329,
          0.9989059583770321,
          0.998304406964278,
          0.9991110483113691,
          0.9988874385501857,
          0.9971959610315406,
          0.9993648385311171,
          0.9989251641233914,
          0.9989162471697246,
          0.9983675115594586,
          0.9993401454286551,
          0.9993538638189118,
          0.9941175542579482,
          0.9971918455144636,
          0.9876411022177836,
          0.9975231446391617,
          0.9993902175530919,
          0.9987742618305683,
          0.9959201507376721,
          0.9913560422992845,
          0.9994848744458628,
          0.9991083046333177,
          0.9988853807916471,
          0.9985040095425123,
          0.9994958491580681,
          0.9979189201980662,
          0.9980657069738122,
          0.9991371132528567,
          0.9929610939593126,
          0.9972185963754641,
          0.9985314463230256,
          0.9991755247455752,
          0.9994286290458105,
          0.9992420589383201,
          0.9993010480164236,
          0.9993312284749883,
          0.9992742971554232,
          0.9983688833984843,
          0.9992598928456538,
          0.9993216256018086,
          0.997153434021745,
          0.9982419882886102,
          0.9994094232994511,
          0.9993415172676807,
          0.9957315228716432,
          0.9993902175530919,
          0.997829064741885,
          0.9993627807725786,
          0.9965868645041454,
          0.9992111925602426,
          0.9835214696237115,
          0.9986171862621296
         ]
        }
       ],
       "layout": {
        "title": "Sample Call Rate Distribution",
        "xaxis": {
         "title": "Call Rate"
        }
       }
      },
      "text/html": [
       "<div class=\"chart\" id=\"plot-f454ab2e-2f2e-48af-a55a-a4f38ee1c686\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[0.9971170802875648,0.9857417910867503,0.99892722188193,0.9957349524692074,0.9797077571323626,0.9981953457617376,0.9933891077353201,0.9974017368853904,0.9963310165258589,0.9987941534964404,0.9987001825231824,0.9987735759110554,0.9989793517649052,0.9985328181620512,0.9982872589764572,0.9987090994768492,0.9984902911522556,0.9973235420609274,0.9985149842547176,0.997026538911871,0.9982070063934558,0.9984703994863835,0.9989450557892636,0.9989347669965711,0.9983496776521249,0.9866053637534065,0.9989697488917255,0.9974717006756993,0.9968365392068164,0.9956231475886157,0.9922120698512995,0.9958309812010039,0.9991858135382677,0.9936058583013752,0.9983778003521511,0.999229712387089,0.9981713385787885,0.9974168271146727,0.9989059583770321,0.9974085960805187,0.9963810886502956,0.9951436898491457,0.9932766169352156,0.996797441794585,0.9993476905432963,0.9977776207784226,0.9979024581297581,0.9988744060794418,0.999143972447985,0.9991035031967279,0.9989217345258272,0.9975416644660082,0.995216397317506,0.9973324590145943,0.9973592098755948,0.9946169036632904,0.9973242279804403,0.9988915540672626,0.9974970796976741,0.9979100032443993,0.9987001825231824,0.9959057464279026,0.9985053813815379,0.9979964291030162,0.9983325296643041,0.9986418793645916,0.9991871853772935,0.9991041891162408,0.9817325915342442,0.9969881274191524,0.9968084165067903,0.9978818805443732,0.9974874768244945,0.9993950189896817,0.9957850245936442,0.9988826371135958,0.9976644440588053,0.9991426006089593,0.9993620948530657,0.9995349465702995,0.9926160764443579,0.9986213017792066,0.9974312314244422,0.9978976566931683,0.9994793870897601,0.9995562100751973,0.9978468986492187,0.9987475109695678,0.9988016986110816,0.9978338661784749,0.9983263563886886,0.998595236837719,0.9993771850823481,0.9983277282277143,0.9943445936166958,0.9992804704310387,0.9946594306730859,0.9985170420132561,0.9991377991723696,0.9995802172581465,0.9993867879555277,0.999410109218964,0.9994780152507344,0.9991734669870368,0.9986919514890283,0.9988668609648007,0.9978578733614241,0.9915714210263139,0.9934398657792697,0.9991961023309602,0.9949879861197327,0.9975773322806755,0.9977542995149863,0.9992427448578329,0.9989059583770321,0.998304406964278,0.9991110483113691,0.9988874385501857,0.9971959610315406,0.9993648385311171,0.9989251641233914,0.9989162471697246,0.9983675115594586,0.9993401454286551,0.9993538638189118,0.9941175542579482,0.9971918455144636,0.9876411022177836,0.9975231446391617,0.9993902175530919,0.9987742618305683,0.9959201507376721,0.9913560422992845,0.9994848744458628,0.9991083046333177,0.9988853807916471,0.9985040095425123,0.9994958491580681,0.9979189201980662,0.9980657069738122,0.9991371132528567,0.9929610939593126,0.9972185963754641,0.9985314463230256,0.9991755247455752,0.9994286290458105,0.9992420589383201,0.9993010480164236,0.9993312284749883,0.9992742971554232,0.9983688833984843,0.9992598928456538,0.9993216256018086,0.997153434021745,0.9982419882886102,0.9994094232994511,0.9993415172676807,0.9957315228716432,0.9993902175530919,0.997829064741885,0.9993627807725786,0.9965868645041454,0.9992111925602426,0.9835214696237115,0.9986171862621296],\"type\":\"histogram\"};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"Sample Call Rate Distribution\",\"xaxis\":{\"title\":\"Call Rate\"}};\n",
       "\n",
       "  Plotly.plot('plot-f454ab2e-2f2e-48af-a55a-a4f38ee1c686', data, layout);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres21\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-f454ab2e-2f2e-48af-a55a-a4f38ee1c686\"\u001b[39m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stat\n",
    "    .selectExpr(\"explode(qc) as qc\")\n",
    "    .fn(d => {\n",
    "        Histogram(\n",
    "            x=d.select(\"qc.callRate\").collect.map(_.getAs[Double](\"callRate\")).toList\n",
    "        ).plot(title=\"Sample Call Rate Distribution\", xaxis=Axis(title=\"Call Rate\"))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there is a way to get the per-sample statistics, we need a way to apply a filtering on this data back to the original dataset.  The current documentation which includes a [notebook](https://glow.readthedocs.io/en/latest/_static/notebooks/etl/sample-qc-demo.html) for doing this appears to be out of date (it suggests using a form of output from ```sample_call_summary_stats``` that no longer exists) but a slight adapatation of this below will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:18:12.961017Z",
     "start_time": "2020-01-16T04:16:23.922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfilterBySampleCallRate\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Filter a genotype dataset to samples with a minimum call rate (across variants)\n",
    "def filterBySampleCallRate(threshold: Double)(df: DataFrame): DataFrame = { \n",
    "    df\n",
    "        // Cross join original dataset with single-row data frame containing a map like (sampleId -> QC stats)\n",
    "        .crossJoin(\n",
    "            df\n",
    "            .selectExpr(\"sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc\")\n",
    "            .selectExpr(\"map_from_arrays(qc.sampleId, qc) as qc\")\n",
    "        )\n",
    "        // For each row, filter the genotypes array (which has one element per sampleId) based on QC map lookup\n",
    "        .selectExpr(\"*\", s\"filter(genotypes, g -> qc[g.sampleId].callRate >= ${threshold}) as filtered_genotypes\")\n",
    "        // Remove intermediate fields \n",
    "        .drop(\"qc\", \"genotypes\").withColumnRenamed(\"filtered_genotypes\", \"genotypes\")\n",
    "        // Ensure that the original dataset schema was preserved\n",
    "        .transform(d => {assert(d.schema.equals(df.schema)); d})\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variant Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:18:13.148203Z",
     "start_time": "2020-01-16T04:16:23.923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- qc: struct (nullable = true)\n",
      " |    |-- callRate: double (nullable = false)\n",
      " |    |-- nCalled: integer (nullable = false)\n",
      " |    |-- nUncalled: integer (nullable = false)\n",
      " |    |-- nHet: integer (nullable = false)\n",
      " |    |-- nHomozygous: array (nullable = true)\n",
      " |    |    |-- element: integer (containsNull = false)\n",
      " |    |-- nNonRef: integer (nullable = false)\n",
      " |    |-- nAllelesCalled: integer (nullable = false)\n",
      " |    |-- alleleCounts: array (nullable = true)\n",
      " |    |    |-- element: integer (containsNull = false)\n",
      " |    |-- alleleFrequencies: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdf_stat\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_stat = df.selectExpr(\"call_summary_stats(genotypes) as qc\")\n",
    "df_stat.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the number of QC results in this case equals the number of variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:18:13.382799Z",
     "start_time": "2020-01-16T04:16:23.925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres24\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1457897L\u001b[39m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stat.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:18:43.396388Z",
     "start_time": "2020-01-16T04:16:23.926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          0.9500000000000001,
          0.96,
          0.965,
          0.97,
          0.975,
          0.98,
          0.99,
          0.995,
          1
         ],
         "y": [
          1207,
          2045,
          3580,
          6849,
          13773,
          30434,
          86563,
          231404,
          1082042
         ]
        }
       ],
       "layout": {
        "title": "Variant Call Rate Distribution",
        "xaxis": {
         "title": "Call Rate"
        },
        "yaxis": {
         "title": "Num Variants",
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div class=\"chart\" id=\"plot-1cd8a685-96b7-4d73-ab70-e022b6d65d6b\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[0.9500000000000001,0.96,0.965,0.97,0.975,0.98,0.99,0.995,1.0],\"y\":[1207.0,2045.0,3580.0,6849.0,13773.0,30434.0,86563.0,231404.0,1082042.0],\"type\":\"bar\"};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"Variant Call Rate Distribution\",\"yaxis\":{\"title\":\"Num Variants\",\"type\":\"log\"},\"xaxis\":{\"title\":\"Call Rate\"}};\n",
       "\n",
       "  Plotly.plot('plot-1cd8a685-96b7-4d73-ab70-e022b6d65d6b', data, layout);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres25\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1cd8a685-96b7-4d73-ab70-e022b6d65d6b\"\u001b[39m"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Technical note: do histogram binning manually with a large number of elements \n",
    "// because plotly-scala will put data provided as json object in cell output\n",
    "df_stat\n",
    "    .withColumn(\"bin\", bround($\"qc.callRate\"/.005)*.005)\n",
    "    .groupBy(\"bin\").count.sort($\"bin\".asc)\n",
    "    .fn(d => {\n",
    "        Bar(\n",
    "            x=d.map(_.getAs[Double](\"bin\")).collect.toList,\n",
    "            y=d.map(_.getAs[Long](\"count\")).collect.toList\n",
    "        ).plot(\n",
    "            title=\"Variant Call Rate Distribution\", \n",
    "            xaxis=Axis(title=\"Call Rate\"),\n",
    "            yaxis=Axis(`type`=AxisType.Log, title=\"Num Variants\")\n",
    "        )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:18:43.543334Z",
     "start_time": "2020-01-16T04:16:23.927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfilterByVariantCallRate\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Filter a genotype dataset to variants with a minimum call rate (across samples)\n",
    "def filterByVariantCallRate(threshold: Double)(df: DataFrame): DataFrame = { \n",
    "    df\n",
    "        .selectExpr(\"*\", \"call_summary_stats(genotypes) as qc\")\n",
    "        .filter($\"qc.callRate\" >= threshold)\n",
    "        .drop(\"qc\")\n",
    "        .transform(d => {assert(d.schema.equals(df.schema)); d})\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Filtering\n",
    "\n",
    "Use the functions defined above to create a filtering operation that replicates that in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:23:01.210405Z",
     "start_time": "2020-01-16T04:16:23.928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 257.3 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_qc1\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [contigName: string, names: array<string> ... 6 more fields]\n",
       "\u001b[36mct\u001b[39m: (\u001b[32mLong\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m1430443L\u001b[39m, \u001b[32m165\u001b[39m)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Invert the .2 and .02 thresholds below but preserve order of filter application\n",
    "val df_qc1 = df\n",
    "    .transform(filterByVariantCallRate(threshold=.8))\n",
    "    .transform(filterBySampleCallRate(threshold=.8))\n",
    "    .transform(filterByVariantCallRate(threshold=.98))\n",
    "    .transform(filterBySampleCallRate(threshold=.98))\n",
    "\n",
    "val ct = timeop(\"qc1\") {\n",
    "    count(df_qc1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T12:21:13.402283Z",
     "start_time": "2020-01-11T12:20:48.331Z"
    }
   },
   "source": [
    "Validate results against tutorial data at this stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:23:02.750412Z",
     "start_time": "2020-01-16T04:16:23.930Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(count(ss.read.format(\"plink\").load(data_dir / QC1_FILE + \".bed\" toString)) == ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Materialize results for following step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:27:42.352686Z",
     "start_time": "2020-01-16T04:16:23.931Z"
    }
   },
   "outputs": [],
   "source": [
    "df_qc1.write.mode(\"overwrite\").parquet(data_dir / QC1_FILE + \".parquet\" toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"step_2\">Step 2: Gender Discrepancy</a></h2>\n",
    "\n",
    "The ```plink``` \"--check-sex\" command uses homozygosity rates on sex chromosomes to determine if the gender assigned to a person (in the ```.fam``` file) is unlikely.  The tutorial documentation states that an \"F value\" is called and that:\n",
    "\n",
    "> This F value is based on the X chromosome inbreeding (homozygosity) estimate.\n",
    "\n",
    "The [plink check-sex documentation](https://www.cog-genomics.org/plink/1.9/basic_stats#check_sex) is similarly vague in its description of this statistic:\n",
    "\n",
    "> --check-sex normally compares sex assignments in the input dataset with those imputed from X chromosome inbreeding coefficients\n",
    "\n",
    "It's not entirely clear what this statistic calculation requires, but the [source code](https://github.com/chrchang/plink-ng/blob/b6bb12d5a14523f53865843104aaa20ed057aff5/1.9/plink_misc.c#L3206) for it looks as though the calculation is close to simply being the homozygosity rate.  The [plink inbreeding coefficient docs](https://www.cog-genomics.org/plink/1.9/basic_stats#ibc) (which the ```--check-sex``` command statistic \"is based on\") may give some more details on exactly what the statistic is (it is lifted from GCTA) but for the purposes of this analysis, homozygosity rate will be used because we know which samples should be omitted and can validate against the ```plink``` results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:27:42.573275Z",
     "start_time": "2020-01-16T04:16:23.933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.parquet(data_dir / QC1_FILE + \".parquet\" toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:27:42.970610Z",
     "start_time": "2020-01-16T04:16:23.934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|contigName| count|\n",
      "+----------+------+\n",
      "|         1|117459|\n",
      "|         2|117501|\n",
      "|         3| 97341|\n",
      "|         4| 86388|\n",
      "|         5| 88819|\n",
      "|         6| 92037|\n",
      "|         7| 75919|\n",
      "|         8| 75777|\n",
      "|         9| 64180|\n",
      "|        10| 74379|\n",
      "|        11| 71812|\n",
      "|        12| 68941|\n",
      "|        13| 52335|\n",
      "|        14| 45626|\n",
      "|        15| 42284|\n",
      "|        16| 45090|\n",
      "|        17| 38729|\n",
      "|        18| 41141|\n",
      "|        19| 26437|\n",
      "|        20| 36624|\n",
      "|        21| 19415|\n",
      "|        22| 20310|\n",
      "|        23| 31490|\n",
      "|        25|   409|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Show variant counts by chromosome (i.e. \"contigName\")\n",
    "df.groupBy(\"contigName\").count.sort($\"contigName\".cast(\"int\").asc).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homozygosity rates on X chromosomes for men are expected to be near 1 because there should be no second X chromsome for which alternate alleles can exist (outside of the pseudoautosomal region).  An analysis of the HapMap Phase III data (the same data being used here) in [Inference of Unexpected Genetic Relatedness among Individuals in HapMap Phase III](https://www.cell.com/ajhg/fulltext/S0002-9297(10)00427-1#secd9962300e927) showed that women have a heterozygous/homozygous ratio on X of about .33 (see Figure 2A).  This means that in the histogram below, men should all cluster near one while women should cluster near 3/(3 + 1) = .75.  There is one clear outlier, which is also what ```plink``` found (see the companion analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:28:03.450113Z",
     "start_time": "2020-01-16T04:16:23.935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "Female",
         "type": "histogram",
         "x": [
          0.7693630025785503,
          0.7692699058763673,
          0.7752719638653858,
          0.7487738072488693,
          0.77988361369924,
          0.7703828542355635,
          0.7839635854341737,
          0.7621382406316661,
          0.7756173625254582,
          0.7670026390257861,
          0.7779792004579715,
          0.7561852063855499,
          0.7824066706979409,
          0.761711898459674,
          0.7568144865623108,
          0.7632214594457157,
          0.7608364052370662,
          0.781940075620373,
          0.7705413051079114,
          0.7675297855440826,
          0.7595584960875373,
          0.7790431352589388,
          0.7833784429231014,
          0.7760305586503262,
          0.7693651903158163,
          0.7665861718352822,
          0.7709115196234336,
          0.7637063671365386,
          0.7696176742113138,
          0.7642208871277003,
          0.7907664895478698,
          0.7813672496025437,
          0.7616316124920331,
          0.768976372944955,
          0.7684870074107057,
          0.7699714013346044,
          0.7741197406889538,
          0.7793305893238889,
          0.7816469837216725,
          0.7699834794764264,
          0.7643488209495963,
          0.7950574931707007,
          0.7646403564608529,
          0.772751818557225,
          0.7654599821860287,
          0.7621757375381485,
          0.7845101781170484,
          0.7972030199738779,
          0.7532682337224467,
          0.7713322520115765,
          0.7715539165818922,
          0.7870529191283908,
          0.7672822044941677,
          0.7535383734614038,
          0.762275544255522,
          0.7732053767199466,
          0.756937160293697,
          0.7913344557034402,
          0.7605477188969374,
          0.7745079304414294,
          0.7765281484305503,
          0.7684558426287857,
          0.785510100368441,
          0.7572954415411024,
          0.7634907519227102,
          0.7513182135823645,
          0.7625704550520651,
          0.7784415914579891,
          0.7823987291501191,
          0.7731781762685997,
          0.7755153902353801,
          0.7672098859557165,
          0.7708975377283558,
          0.7745892522324975,
          0.7684534968701344,
          0.7693065531049387,
          0.7683726612229151,
          0.7837030445560287,
          0.7794720289716954,
          0.770210868318787,
          0.7724392651319769,
          0.764864778974799,
          0.7630834736741763,
          0.9982031397768111,
          0.7791289952884248
         ],
         "xbins": {
          "end": 1.01,
          "size": 0.01,
          "start": 0.7
         }
        },
        {
         "name": "Male",
         "type": "histogram",
         "x": [
          1,
          1,
          1,
          1,
          0.999968192372531,
          1,
          0.9999682277435343,
          1,
          0.9999046650565654,
          1,
          0.9999682085518995,
          1,
          1,
          0.9999364373112982,
          1,
          0.9999681903489519,
          1,
          1,
          1,
          1,
          1,
          1,
          0.9999681994530306,
          0.9999363320918091,
          1,
          1,
          0.9999045801526718,
          1,
          0.9999681933842239,
          1,
          1,
          1,
          0.9938193209719752,
          1,
          1,
          1,
          1,
          1,
          1,
          0.9999682247148168,
          0.9999681812396589,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0.9999682317809263,
          1,
          1,
          0.9995206748897553,
          1,
          0.99881955079122,
          1,
          1,
          1,
          0.9999682317809263,
          0.999840911260301,
          0.9992923086820857,
          1,
          1,
          1,
          1,
          0.9999364312503973,
          1,
          1,
          0.9998411437648927,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "xbins": {
          "end": 1.01,
          "size": 0.01,
          "start": 0.7
         }
        }
       ],
       "layout": {
        "title": "Sex Chromosome Homozygosity Rates",
        "xaxis": {
         "range": [
          0.7,
          1.01
         ],
         "title": "Homozygosity Rate"
        },
        "yaxis": {
         "title": "Sample Count",
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div class=\"chart\" id=\"plot-5f31b362-28c0-4e47-9546-400f80e2bff3\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[0.7693630025785503,0.7692699058763673,0.7752719638653858,0.7487738072488693,0.77988361369924,0.7703828542355635,0.7839635854341737,0.7621382406316661,0.7756173625254582,0.7670026390257861,0.7779792004579715,0.7561852063855499,0.7824066706979409,0.761711898459674,0.7568144865623108,0.7632214594457157,0.7608364052370662,0.781940075620373,0.7705413051079114,0.7675297855440826,0.7595584960875373,0.7790431352589388,0.7833784429231014,0.7760305586503262,0.7693651903158163,0.7665861718352822,0.7709115196234336,0.7637063671365386,0.7696176742113138,0.7642208871277003,0.7907664895478698,0.7813672496025437,0.7616316124920331,0.768976372944955,0.7684870074107057,0.7699714013346044,0.7741197406889538,0.7793305893238889,0.7816469837216725,0.7699834794764264,0.7643488209495963,0.7950574931707007,0.7646403564608529,0.772751818557225,0.7654599821860287,0.7621757375381485,0.7845101781170484,0.7972030199738779,0.7532682337224467,0.7713322520115765,0.7715539165818922,0.7870529191283908,0.7672822044941677,0.7535383734614038,0.762275544255522,0.7732053767199466,0.756937160293697,0.7913344557034402,0.7605477188969374,0.7745079304414294,0.7765281484305503,0.7684558426287857,0.785510100368441,0.7572954415411024,0.7634907519227102,0.7513182135823645,0.7625704550520651,0.7784415914579891,0.7823987291501191,0.7731781762685997,0.7755153902353801,0.7672098859557165,0.7708975377283558,0.7745892522324975,0.7684534968701344,0.7693065531049387,0.7683726612229151,0.7837030445560287,0.7794720289716954,0.770210868318787,0.7724392651319769,0.764864778974799,0.7630834736741763,0.9982031397768111,0.7791289952884248],\"name\":\"Female\",\"xbins\":{\"size\":0.01,\"end\":1.01,\"start\":0.7},\"type\":\"histogram\"};\n",
       "  var data1 = {\"x\":[1.0,1.0,1.0,1.0,0.999968192372531,1.0,0.9999682277435343,1.0,0.9999046650565654,1.0,0.9999682085518995,1.0,1.0,0.9999364373112982,1.0,0.9999681903489519,1.0,1.0,1.0,1.0,1.0,1.0,0.9999681994530306,0.9999363320918091,1.0,1.0,0.9999045801526718,1.0,0.9999681933842239,1.0,1.0,1.0,0.9938193209719752,1.0,1.0,1.0,1.0,1.0,1.0,0.9999682247148168,0.9999681812396589,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9999682317809263,1.0,1.0,0.9995206748897553,1.0,0.99881955079122,1.0,1.0,1.0,0.9999682317809263,0.999840911260301,0.9992923086820857,1.0,1.0,1.0,1.0,0.9999364312503973,1.0,1.0,0.9998411437648927,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"name\":\"Male\",\"xbins\":{\"size\":0.01,\"end\":1.01,\"start\":0.7},\"type\":\"histogram\"};\n",
       "\n",
       "  var data = [data0, data1];\n",
       "  var layout = {\"title\":\"Sex Chromosome Homozygosity Rates\",\"yaxis\":{\"title\":\"Sample Count\",\"type\":\"log\"},\"xaxis\":{\"range\":[0.7,1.01],\"title\":\"Homozygosity Rate\"}};\n",
       "\n",
       "  Plotly.plot('plot-5f31b362-28c0-4e47-9546-400f80e2bff3', data, layout);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdf_gender_impute\u001b[39m\n",
       "\u001b[36mres32_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-5f31b362-28c0-4e47-9546-400f80e2bff3\"\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_gender_impute = df\n",
    "    // Filter to X/Y chromosome\n",
    "    .filter($\"contigName\" === \"23\")\n",
    "    // Get per-sample call statistics\n",
    "    .selectExpr(\"sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc\")\n",
    "    .selectExpr(\"explode(qc) as qc\")\n",
    "    // Compute homozygosity rates for each person\n",
    "    // See: https://github.com/projectglow/glow/blob/75ea111e0ba86001ea213df103373125b8732778/core/src/main/scala/io/projectglow/sql/expressions/SampleCallSummaryStats.scala#L162\n",
    "    // for calculation\n",
    "    .select($\"qc.sampleId\", (lit(1.0) - $\"qc.nHet\".cast(\"double\") / $\"qc.nCalled\".cast(\"double\")).as(\"homRate\"))\n",
    "    // Grab current gender assignment\n",
    "    .join(\n",
    "        dp.select($\"sampleId\", $\"gender\"),\n",
    "        Seq(\"sampleId\")\n",
    "    )\n",
    "    // Add imputed gender assignment\n",
    "    .withColumn(\"imputedGender\", when($\"homRate\" > .9 && $\"gender\" === \"2\", \"1\").otherwise($\"gender\"))\n",
    "\n",
    "df_gender_impute.fn(d => {\n",
    "        // Create histogram trace for each gender\n",
    "        d.select(\"gender\").dropDuplicates().map(_.getAs[String](\"gender\")).collect.map(g => \n",
    "            Histogram(\n",
    "                x=d.filter($\"gender\" === g).map(_.getAs[Double](\"homRate\")).collect.toList, \n",
    "                // See https://www.cog-genomics.org/plink/1.9/formats#fam for encoding\n",
    "                name=Map(\"1\" -> \"Male\", \"2\" -> \"Female\")(g),\n",
    "                xbins=Bins(0.7, 1.01, .01)\n",
    "            )\n",
    "        ).toSeq.plot(\n",
    "            title=\"Sex Chromosome Homozygosity Rates\",\n",
    "            yaxis=Axis(`type`=AxisType.Log, title=\"Sample Count\"),\n",
    "            xaxis=Axis(title=\"Homozygosity Rate\", range=(0.7, 1.01))\n",
    "        )\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the single female sample above the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:28:10.189462Z",
     "start_time": "2020-01-16T04:16:23.937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------+-------------+\n",
      "|    sampleId|           homRate|gender|imputedGender|\n",
      "+------------+------------------+------+-------------+\n",
      "|1349_NA10854|0.9982031397768111|     2|            1|\n",
      "+------------+------------------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gender_impute.filter($\"gender\" =!= $\"imputedGender\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the transformed version of the demographic data up through step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:28:23.781089Z",
     "start_time": "2020-01-16T04:16:23.938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "(1430443,165)\n",
      "Elapsed time: 6.9 seconds\n",
      "+------------+----+-------+----+----+------+---------+\n",
      "|sampleId    |fid |iid    |iidp|iidm|gender|phenotype|\n",
      "+------------+----+-------+----+----+------+---------+\n",
      "|1328_NA06989|1328|NA06989|0   |0   |2     |2        |\n",
      "|1377_NA11891|1377|NA11891|0   |0   |1     |2        |\n",
      "|1349_NA11843|1349|NA11843|0   |0   |1     |1        |\n",
      "+------------+----+-------+----+----+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdp_qc2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [sampleId: string, fid: string ... 5 more fields]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This only affects the pedigree data, not the genotype dataset\n",
    "val dp_qc2 = timeop(\"qc2\") {\n",
    "    dp\n",
    "        .join(df_gender_impute.select(\"sampleId\", \"imputedGender\"), Seq(\"sampleId\"), \"left\")\n",
    "        .withColumn(\"gender\", when($\"gender\" =!= $\"imputedGender\", $\"imputedGender\").otherwise($\"gender\"))\n",
    "        .drop(\"imputedGender\")\n",
    "        .transform(d => {println(d.count); d})\n",
    "        // While somewhat awkward here, this count of the genotype data is included to align\n",
    "        // with what other tutorial do in this step\n",
    "        .transform(d => {println(count(df)); d})\n",
    "}\n",
    "dp_qc2.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:28:30.717211Z",
     "start_time": "2020-01-16T04:16:23.939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+\n",
      "|gender|new_count|old_count|\n",
      "+------+---------+---------+\n",
      "|     2|       84|       85|\n",
      "|     1|       81|       80|\n",
      "+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dp_qc2.groupBy(\"gender\").count.withColumnRenamed(\"count\", \"new_count\").join(\n",
    "    dp.groupBy(\"gender\").count.withColumnRenamed(\"count\", \"old_count\"),\n",
    "    Seq(\"gender\")\n",
    ").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:28:37.315959Z",
     "start_time": "2020-01-16T04:16:23.940Z"
    }
   },
   "outputs": [],
   "source": [
    "dp_qc2.write.mode(\"overwrite\").parquet(data_dir / QC2_FILE + \".fam.parquet\" toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:28:37.513870Z",
     "start_time": "2020-01-16T04:16:23.942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdp_qc2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [sampleId: string, fid: string ... 5 more fields]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dp_qc2 = ss.read.parquet(data_dir / QC2_FILE + \".fam.parquet\" toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T23:33:25.420776Z",
     "start_time": "2020-01-10T23:33:25.326Z"
    }
   },
   "source": [
    "<h2><a id=\"step_3\">Step 3: Autosomal Variants and MAF Filtering</a></h2>\n",
    "\n",
    "This step will remove variants for nonautosomal chromosomes as well SNPs that are too rare to be useful.\n",
    "\n",
    "Regarding minor allele frequency (MAF) filtering, the tutorial states:\n",
    "\n",
    "> The MAF threshold should depend on your sample size, larger samples can use lower MAF thresholds. Respectively, for large (N = 100.000) vs. moderate samples (N = 10000), 0.01 and 0.05 are commonly used as MAF threshold.\n",
    "\n",
    "This dataset has N=165 (individuals) so the more aggressive 0.05 threshold will be used.\n",
    "\n",
    "Additionally, ```plink``` defaults to calculating MAF only for population \"founders\".  The docs at [plink#maf](http://zzz.bwh.harvard.edu/plink/thresh.shtml#maf) state:\n",
    "\n",
    "> This quantity is based only on founders (i.e. individuals for whom the paternal and maternal individual codes and both 0).\n",
    "\n",
    "This can be disabled using the ```--nonfounders``` flag, which makes the filtering below equivalent to ignoring everything about samples entirely, but we will instead use the ```.fam``` file data to identify founders the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:29:01.483084Z",
     "start_time": "2020-01-16T04:29:01.231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.parquet(data_dir / QC1_FILE + \".parquet\" toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:29:36.941037Z",
     "start_time": "2020-01-16T04:29:01.513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          0,
          0.01,
          0.02,
          0.03,
          0.04,
          0.05,
          0.06,
          0.07,
          0.08,
          0.09,
          0.1,
          0.11,
          0.12,
          0.13,
          0.14,
          0.15,
          0.16,
          0.17,
          0.18,
          0.19,
          0.2,
          0.21,
          0.22,
          0.23,
          0.24,
          0.25,
          0.26,
          0.27,
          0.28,
          0.29,
          0.3,
          0.31,
          0.32,
          0.33,
          0.34,
          0.35000000000000003,
          0.36,
          0.37,
          0.38,
          0.39,
          0.4,
          0.41000000000000003,
          0.42,
          0.43,
          0.44,
          0.45,
          0.46,
          0.47000000000000003,
          0.48,
          0.49,
          0.5
         ],
         "y": [
          26546,
          36449,
          32907,
          24378,
          25505,
          34172,
          27444,
          27805,
          34003,
          27527,
          27081,
          26329,
          33669,
          25782,
          25015,
          30710,
          25612,
          24258,
          29478,
          23290,
          24356,
          23612,
          28257,
          23956,
          22512,
          27328,
          21500,
          22742,
          26125,
          21788,
          22373,
          20664,
          26173,
          21066,
          21506,
          25499,
          20256,
          20333,
          25836,
          19451,
          20399,
          21005,
          24652,
          20014,
          19909,
          25107,
          19747,
          19338,
          26153,
          19538,
          9737
         ]
        }
       ],
       "layout": {
        "margin": {
         "b": 150
        },
        "title": "MAF Distribution",
        "xaxis": {
         "title": "Minor Allele Frequency (MAF)"
        },
        "yaxis": {
         "title": "Num Variants",
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div class=\"chart\" id=\"plot-505fa71c-f34d-4450-81e5-ff007ff8c7a2\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[0.0,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35000000000000003,0.36,0.37,0.38,0.39,0.4,0.41000000000000003,0.42,0.43,0.44,0.45,0.46,0.47000000000000003,0.48,0.49,0.5],\"y\":[26546.0,36449.0,32907.0,24378.0,25505.0,34172.0,27444.0,27805.0,34003.0,27527.0,27081.0,26329.0,33669.0,25782.0,25015.0,30710.0,25612.0,24258.0,29478.0,23290.0,24356.0,23612.0,28257.0,23956.0,22512.0,27328.0,21500.0,22742.0,26125.0,21788.0,22373.0,20664.0,26173.0,21066.0,21506.0,25499.0,20256.0,20333.0,25836.0,19451.0,20399.0,21005.0,24652.0,20014.0,19909.0,25107.0,19747.0,19338.0,26153.0,19538.0,9737.0],\"type\":\"bar\"};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"margin\":{\"b\":150},\"title\":\"MAF Distribution\",\"yaxis\":{\"title\":\"Num Variants\",\"type\":\"log\"},\"xaxis\":{\"title\":\"Minor Allele Frequency (MAF)\"}};\n",
       "\n",
       "  Plotly.plot('plot-505fa71c-f34d-4450-81e5-ff007ff8c7a2', data, layout);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres39\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-505fa71c-f34d-4450-81e5-ff007ff8c7a2\"\u001b[39m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Show MAF ignoring founder status\n",
    "df\n",
    "    .selectExpr(\"call_summary_stats(genotypes) as qc\")\n",
    "    .filter(size($\"qc.alleleFrequencies\") > 1)\n",
    "    .withColumn(\"maf\", array_min($\"qc.alleleFrequencies\"))\n",
    "    .withColumn(\"bin\", bround($\"maf\"/.01)*.01)\n",
    "    .groupBy(\"bin\").count.sort($\"bin\".asc)\n",
    "    .fn(d => {\n",
    "        Bar(\n",
    "            x=d.map(_.getAs[Double](\"bin\")).collect.toList,\n",
    "            y=d.map(_.getAs[Long](\"count\")).collect.toList\n",
    "        ).plot(\n",
    "            title=\"MAF Distribution\",\n",
    "            yaxis=Axis(`type`=AxisType.Log, title=\"Num Variants\"),\n",
    "            xaxis=Axis(title=\"Minor Allele Frequency (MAF)\"),\n",
    "            margin=Margin(b=150)\n",
    "        )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:31:26.327129Z",
     "start_time": "2020-01-16T04:29:50.842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 95.0 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_qc3\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]\n",
       "\u001b[36mct\u001b[39m: (\u001b[32mLong\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m1073226L\u001b[39m, \u001b[32m165\u001b[39m)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_qc3 = df\n",
    "    // Limit to autosomal chromosomes\n",
    "    .filter($\"contigName\".cast(\"int\").between(1, 22))\n",
    "    // Cross join to single-row dataset with founders map\n",
    "    .crossJoin(\n",
    "        dp_qc2\n",
    "            .withColumn(\"isFounder\", $\"iidp\" === \"0\" && $\"iidm\" === \"0\")\n",
    "            .select(\"sampleId\", \"isFounder\")\n",
    "            .agg(map_from_arrays(collect_list($\"sampleId\"), collect_list($\"isFounder\")).as(\"isFounder\"))\n",
    "    )\n",
    "    // Run call_summary_stats only for population founders\n",
    "    .selectExpr(\"*\", \"call_summary_stats(filter(genotypes, g -> isFounder[g.sampleId])) as qc\")\n",
    "    // Ensure there are at least two alleles and if so, that MAF >= 5%\n",
    "    .filter(size($\"qc.alleleFrequencies\") > 1 && array_min($\"qc.alleleFrequencies\") >= 0.05)\n",
    "    // Remove intermediate fields\n",
    "    .drop(\"qc\", \"isFounder\")\n",
    "\n",
    "val ct = timeop(\"qc3\") {\n",
    "    count(df_qc3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:32:21.431896Z",
     "start_time": "2020-01-16T04:32:20.349Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(count(ss.read.format(\"plink\").load(data_dir / QC3_FILE + \".bed\" toString)) == ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:34:13.308712Z",
     "start_time": "2020-01-16T04:32:25.001Z"
    }
   },
   "outputs": [],
   "source": [
    "df_qc3.write.mode(\"overwrite\").parquet(data_dir / QC3_FILE + \".parquet\" toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"step_4\">Step 4: Hardy-Weinberg Equilibrium Filtering</a></h2>\n",
    "\n",
    "This step will identify and eliminate SNPs that are not in Hardy-Weingberg equilibrium.  This happens in two phases\n",
    "in the tutorial where in the first, SNPs for elimination are chosen using a less stringent p-value threshold (1e-6) based only on founders in the control group and in the second, SNPs across all individuals are chosen using a more stringent p-value threshold (1e-10).\n",
    "\n",
    "The ```plink``` command for this is described at [plink#hwe](https://www.cog-genomics.org/plink/1.9/filter#hwe), which states that:\n",
    "\n",
    "> Only founders are considered by this test; use --nonfounders to change this. Also, with case/control data, cases and missing phenotypes are normally ignored; override this with 'include-nonctrl'.\n",
    "\n",
    "and:\n",
    "\n",
    "> --hwe's 'midp' modifier applies the mid-p adjustment described in Graffelman J, Moreno V (2013) The mid p-value in exact tests for Hardy-Weinberg equilibrium. The mid-p adjustment tends to bring the null rejection rate in line with the nominal p-value, and also reduces the filter's tendency to favor retention of variants with missing data. We recommend its use.\n",
    "\n",
    "The latter caveat is important because the Glow function ```hardy_weinberg``` (shown in [Variant QC](https://glow.readthedocs.io/en/latest/etl/variant-qc.html)) uses this implementation by default.  This can be seen in the source code for the [HWE function](https://github.com/projectglow/glow/blob/75ea111e0ba86001ea213df103373125b8732778/core/src/main/scala/io/projectglow/sql/expressions/VariantQcExprs.scala#L45) which uses [LeveneHaldane.scala](https://github.com/projectglow/glow/blob/75ea111e0ba86001ea213df103373125b8732778/core/src/main/scala/io/projectglow/sql/util/LeveneHaldane.scala#L26).  That implementation, take from Hail, is best documented in [hail/docs/misc/LeveneHaldane.pdf](https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/misc/LeveneHaldane.pdf) and details the distribution used as part of the 2013 Graffelman J, Moreno V publication [The mid p-value in exact tests for Hardy-Weinberg equilibrium](https://www.ncbi.nlm.nih.gov/pubmed/23934608)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:34:26.407840Z",
     "start_time": "2020-01-16T04:34:26.113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.parquet(data_dir / QC3_FILE + \".parquet\" toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:34:27.011556Z",
     "start_time": "2020-01-16T04:34:26.814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hwe: struct (nullable = true)\n",
      " |    |-- hetFreqHwe: double (nullable = true)\n",
      " |    |-- pValueHwe: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"hardy_weinberg(genotypes) as hwe\")).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:34:53.880172Z",
     "start_time": "2020-01-16T04:34:28.137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          -9.700000000000001,
          -9.600000000000001,
          -8.9,
          -7.9,
          -7.7,
          -7.5,
          -7.4,
          -7,
          -6.800000000000001,
          -6.7,
          -6.4,
          -6,
          -5.9,
          -5.7,
          -5.6000000000000005,
          -5.4,
          -5.300000000000001,
          -5.2,
          -5.1000000000000005,
          -4.9,
          -4.800000000000001,
          -4.7,
          -4.6000000000000005,
          -4.5,
          -4.4,
          -4.3,
          -4.2,
          -4.1000000000000005,
          -4,
          -3.9000000000000004,
          -3.8000000000000003,
          -3.7,
          -3.6,
          -3.5,
          -3.4000000000000004,
          -3.3000000000000003,
          -3.2,
          -3.1,
          -3,
          -2.9000000000000004,
          -2.8000000000000003,
          -2.7,
          -2.6,
          -2.5,
          -2.4000000000000004,
          -2.3000000000000003,
          -2.2,
          -2.1,
          -2,
          -1.9000000000000001,
          -1.8,
          -1.7000000000000002,
          -1.6,
          -1.5,
          -1.4000000000000001,
          -1.3,
          -1.2000000000000002,
          -1.1,
          -1,
          -0.9,
          -0.8,
          -0.7000000000000001,
          -0.6000000000000001,
          -0.5,
          -0.4,
          -0.30000000000000004,
          -0.2,
          -0.1,
          0
         ],
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          1,
          2,
          1,
          6,
          6,
          6,
          3,
          5,
          8,
          9,
          10,
          5,
          14,
          14,
          8,
          25,
          28,
          34,
          32,
          71,
          55,
          106,
          94,
          143,
          214,
          202,
          499,
          289,
          399,
          614,
          637,
          966,
          1147,
          1328,
          1746,
          2764,
          2702,
          3987,
          4253,
          5342,
          7807,
          9191,
          13062,
          14643,
          18438,
          24797,
          32688,
          33415,
          47062,
          66303,
          77307,
          99465,
          118491,
          173297,
          198203,
          111268
         ]
        }
       ],
       "layout": {
        "title": "HWE P-Value Distribution",
        "xaxis": {
         "title": "Log10(p)"
        },
        "yaxis": {
         "title": "Num Variants",
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div class=\"chart\" id=\"plot-1f6fb909-95d5-4430-a9f7-d2a59f556eab\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[-9.700000000000001,-9.600000000000001,-8.9,-7.9,-7.7,-7.5,-7.4,-7.0,-6.800000000000001,-6.7,-6.4,-6.0,-5.9,-5.7,-5.6000000000000005,-5.4,-5.300000000000001,-5.2,-5.1000000000000005,-4.9,-4.800000000000001,-4.7,-4.6000000000000005,-4.5,-4.4,-4.3,-4.2,-4.1000000000000005,-4.0,-3.9000000000000004,-3.8000000000000003,-3.7,-3.6,-3.5,-3.4000000000000004,-3.3000000000000003,-3.2,-3.1,-3.0,-2.9000000000000004,-2.8000000000000003,-2.7,-2.6,-2.5,-2.4000000000000004,-2.3000000000000003,-2.2,-2.1,-2.0,-1.9000000000000001,-1.8,-1.7000000000000002,-1.6,-1.5,-1.4000000000000001,-1.3,-1.2000000000000002,-1.1,-1.0,-0.9,-0.8,-0.7000000000000001,-0.6000000000000001,-0.5,-0.4,-0.30000000000000004,-0.2,-0.1,0.0],\"y\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,6.0,6.0,6.0,3.0,5.0,8.0,9.0,10.0,5.0,14.0,14.0,8.0,25.0,28.0,34.0,32.0,71.0,55.0,106.0,94.0,143.0,214.0,202.0,499.0,289.0,399.0,614.0,637.0,966.0,1147.0,1328.0,1746.0,2764.0,2702.0,3987.0,4253.0,5342.0,7807.0,9191.0,13062.0,14643.0,18438.0,24797.0,32688.0,33415.0,47062.0,66303.0,77307.0,99465.0,118491.0,173297.0,198203.0,111268.0],\"type\":\"bar\"};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"HWE P-Value Distribution\",\"yaxis\":{\"title\":\"Num Variants\",\"type\":\"log\"},\"xaxis\":{\"title\":\"Log10(p)\"}};\n",
       "\n",
       "  Plotly.plot('plot-1f6fb909-95d5-4430-a9f7-d2a59f556eab', data, layout);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres45\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1f6fb909-95d5-4430-a9f7-d2a59f556eab\"\u001b[39m"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "    .select(expr(\"log10(hardy_weinberg(genotypes).pValueHwe)\").as(\"p\"))\n",
    "    .withColumn(\"bin\", bround($\"p\"/.1)*.1)\n",
    "    .groupBy(\"bin\").count.sort($\"bin\".asc)\n",
    "    .fn(d => {\n",
    "        Bar(\n",
    "            x=d.map(_.getAs[Double](\"bin\")).collect.toList,\n",
    "            y=d.map(_.getAs[Long](\"count\")).collect.toList\n",
    "        ).plot(\n",
    "            title=\"HWE P-Value Distribution\",\n",
    "            yaxis=Axis(`type`=AxisType.Log, title=\"Num Variants\"),\n",
    "            xaxis=Axis(title=\"Log10(p)\")\n",
    "        )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:35:24.785695Z",
     "start_time": "2020-01-16T04:34:34.308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 30.7 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_qc4\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]\n",
       "\u001b[36mct\u001b[39m: (\u001b[32mLong\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m1073226L\u001b[39m, \u001b[32m165\u001b[39m)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_qc4 = df\n",
    "    // Phase 1: controls + founders only, less stringent threshold\n",
    "    .crossJoin(\n",
    "        dp_qc2\n",
    "            .withColumn(\"isFounder\", $\"iidp\" === \"0\" && $\"iidm\" === \"0\")\n",
    "            .withColumn(\"isControl\", $\"phenotype\" === \"1\") // 1=control, 2=case, -9=missing\n",
    "            .filter($\"isFounder\" && $\"isControl\")\n",
    "            .agg(map_from_arrays(collect_list($\"sampleId\"), collect_list(lit(true))).as(\"isTarget\"))\n",
    "    )\n",
    "    .withColumn(\"hwe\", expr(\"hardy_weinberg(filter(genotypes, g -> coalesce(isTarget[g.sampleId], false)))\"))\n",
    "    .filter($\"hwe.pValueHwe\" > 1e-6)\n",
    "    .drop(\"isTarget\", \"hwe\")\n",
    "\n",
    "    // Phase 2: all samples, more stringent threshold\n",
    "    .withColumn(\"hwe\", expr(\"hardy_weinberg(genotypes)\"))\n",
    "    .filter($\"hwe.pValueHwe\" > 1e-10)\n",
    "    .drop(\"hwe\")\n",
    "\n",
    "val ct = timeop(\"qc4\") {\n",
    "    count(df_qc4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:36:20.688558Z",
     "start_time": "2020-01-16T04:36:19.701Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(count(ss.read.format(\"plink\").load(data_dir / QC4_FILE + \".bed\" toString)) == ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:37:05.666857Z",
     "start_time": "2020-01-16T04:36:21.629Z"
    }
   },
   "outputs": [],
   "source": [
    "df_qc4.write.mode(\"overwrite\").parquet(data_dir / QC4_FILE + \".parquet\" toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"step_5\">Step 5: Heterozygosity Filtering</a></h2>\n",
    "\n",
    "This step is simple in principle.  The objective is to identify and remove samples with an unusually large or small heterozygosity rate.  One difficulty that arises when doing this though is that estimating this rate is often very biased due to LD.  The most common approach for dealing with this issue is to select a single variant from a group of variants in high LD and estimate the rate using only these representative variants.  Another difficulty is that genomic regions with frequent inversion mutations result in very different LD relationships for SNPs near the breakpoints.  Both are detailed more below.\n",
    "\n",
    "#### Variant Pruning/Clumping Primer\n",
    "\n",
    "There are several approaches that may be taken for this kind of representative variant election and two common ones are known as \"pruning\" and \"clumping\".  Pruning is supported by both ```plink``` and Hail (via [ld_prune](https://hail.is/docs/0.2/_modules/hail/methods/statgen.html#ld_prune)).  This process works by ordering the variants along the genome, moving in windows of at most some base pair block size, determining the correlation between all variants in that window, and greedily removing any with a correlation above some threshold.  The (now deprecated) Hail 0.1 docs for ld_prune show a simplified version of their algorithm (the 0.2 [ld_prune](https://hail.is/docs/0.2/methods/genetics.html?highlight=ld_prune#hail.methods.ld_prune) algorithm is substantially more complicated):\n",
    "\n",
    "```python\n",
    "pruned_set = []\n",
    "for v1 in contig:\n",
    "    keep = True\n",
    "    for v2 in pruned_set:\n",
    "        if ((v1.position - v2.position) <= window and correlation(v1, v2) >= r2):\n",
    "            keep = False\n",
    "    if keep:\n",
    "        pruned_set.append(v1)\n",
    "```\n",
    "\n",
    "In ```plink```, a command to do this often looks like ```plink --bfile my-data --indep-pairwise 50 5 0.2```.  The ```--indep-pairwise``` flag tells ```plink``` to \"produce a pruned subset of markers that are in approximate linkage equilibrium with each other, writing the IDs to plink.prune.in (and the IDs of all excluded variants to plink.prune.out)\" (see [plink#LD](https://www.cog-genomics.org/plink/1.9/ld#indep)).\n",
    "\n",
    "Glow provides no utilities for performing operations like this.  Until this point in the tutorial, it has been fairly straightforward to find equivalent or near-equivalent Spark operators for what ```plink``` does but iterative algorithms utilizing both genomic distance and pairwise correlation are not easy to concisely replicate in this context.\n",
    "\n",
    "The \"clumping\" approach, however, might constitute a method that is easier to replicate and this is [post](https://privefl.github.io/bigsnpr/articles/pruning-vs-clumping.html#clumping) by Florian PrivÃ© demonstrates the difference well.  This is a an example from the [bigsnpr](https://github.com/privefl/bigsnpr) R package ([c++ clumping implementation](https://github.com/privefl/bigsnpr/blob/321d45ec9c8e2c9d1ec5576ac279fefbec6e7159/src/clumping.cpp)) with an accompanying manuscript by Florian called [Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr](https://doi.org/10.1093/bioinformatics/bty185).  He describes the difference between pruning and clumping [here](https://www.biostars.org/p/343818/) as:\n",
    "\n",
    "> 1. pruning: it uses the first SNP (in genome order) and computes the correlation with the following ones (e.g. 50). When it finds a large correlation, it removes one SNP from the correlated pair, keeping the one with the largest minor allele frequency (MAF), thus possibly removing the first SNP. Then it goes on with the next SNP (not yet removed). So, in some worst case scenario, this algorithm may in fact remove all SNPs of the genome (expect one).\n",
    "\n",
    "> 2. clumping: it uses some statistic (usually p-value in the case of GWAS/PRS) to sort the SNPs by importance (e.g. keeping the most significant ones). It takes the first one (e.g. most significant SNP) and removes SNPs that are too correlated with this one in a window around it. As opposed to pruning, this procedure makes sure that this SNP is never removed, keeping at least one representative SNP by region of the genome. Then it goes on with the next most significant SNP that has not been removed yet. In the case of computing principal components, there is no p-value available, so I propose to use the MAF instead as the statistic to rank SNPs (in decreasing order). Using MAFs makes clumping very similar to pruning, but without any worst-case scenario.\n",
    "\n",
    "One of these methods are often used as precursor steps to objectives like:\n",
    "\n",
    "- Variant zygosity statistics (as in the current step)\n",
    "- Genetic relatedness (e.g. Identity by descent (IBD))\n",
    "  - The [plink docs for IBD](https://www.cog-genomics.org/plink/1.9/ibd) immediately caution that:\n",
    "  > These calculations are not LD-aware. It is usually a good idea to perform some form of LD-based pruning before invoking them\n",
    "- Using PCA to identify population substructure\n",
    "  - This [notebook](https://privefl.github.io/bigsnpr/articles/how-to-PCA.html) shows to use pruning prior to PCA\n",
    "- Building a polygenic risk score (PRS) model\n",
    "\n",
    "Because \"LD-aware\" analysis is important and the variant election algorithm needed for it may need to have different priorities, it is worth noting that both approaches make it possible to use some measure of importance for election.  It is possible to do something like this in with ```plink``` as mentioned in this [issue](https://github.com/chrchang/plink-ng/issues/27) by overriding the MAF estimates used to choose between two correlated variants.  A similar custom score could be defined for sorting in the clumping approach too.  This would make it possible to either prioritize variants based on association with an outcome phenotype (not appropriate for PCA prior to running GWAS regression models) or to prioritize them based on being in coding regions, as a couple examples.\n",
    "\n",
    "#### Inversion Mutations\n",
    "\n",
    "When an inversion occurs, SNPs near the breakpoints that were physically close often loose linkage with one another (and gain it with new ones).  This is described in [Evidence for large inversion polymorphisms in the human genome from HapMap data](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1781354/#!po=16.0714) (Bansal et al. 2007):\n",
    "\n",
    "> Consider a genomic region that is inverted (with respect to the reference sequence) in a majority of the chromosomes in a population, and assume that we have genotyped markers on either side of the two breakpoints. In such a scenario, we would expect to see unusually higher levels of long-range LD than would be expected between markers that are physically distant. Furthermore, one would also observe low LD between pairs of markers that are physically close according to the reference sequence. \n",
    "\n",
    "The GWAS tutorial publication addresses this by simply ignoring a few regions where inversions are known to occur frequently (on chromsomes 6 (HLA), 8, and 17).\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "**Aside on Hail ld_prune**: I had hoped that perhaps the Hail implementation for LD pruning would be efficient by comparison to ```plink``` (before looking more at how to do it with Spark directly) and encountered [LD prune performance is unacceptable](https://github.com/hail-is/hail/issues/4506).  I was somewhat encouraged by the later PR [make ld_prune fast again](https://github.com/hail-is/hail/pull/5078) and because this didn't end in a very clear result I tried it myself on the tutorial data.  I found that ```plink``` takes about 2 seconds to do what takes ~4 minutes with ```hl.ld_prune```.\n",
    "\n",
    "For now, this step of the workflow will at least show how to use the Glow [Pipe Transformer](https://glow.readthedocs.io/en/latest/tertiary/pipe-transformer.html) to run ```plink``` commands for pruning prior to removing variants with abnormal heterozygosity rates.  ```plink``` is not built for streaming operations though so there are some awkward accomodations that need to be made, all shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:38:37.047576Z",
     "start_time": "2020-01-16T04:38:36.782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.parquet(data_dir / QC4_FILE + \".parquet\" toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:38:40.389612Z",
     "start_time": "2020-01-16T04:38:40.128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mvariants_to_keep\u001b[39m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get list of variants to use for heterozygosity rate estimation\n",
    "def variants_to_keep = df\n",
    "    // LD pruning in plink is based only on founders so apply the same filtering here\n",
    "    // see: https://www.cog-genomics.org/plink/1.9/ld#indep\n",
    "    .crossJoin(\n",
    "        dp_qc2\n",
    "            .withColumn(\"isFounder\", $\"iidp\" === \"0\" && $\"iidm\" === \"0\")\n",
    "            .select(\"sampleId\", \"isFounder\")\n",
    "            .agg(map_from_arrays(collect_list($\"sampleId\"), collect_list($\"isFounder\")).as(\"isFounder\"))\n",
    "    )\n",
    "    .withColumn(\"genotypes\", expr(\"filter(genotypes, g -> coalesce(isFounder[g.sampleId], false))\"))\n",
    "\n",
    "    // Remove three genome regions with high-frequency inversion mutations\n",
    "    // * equivalent to --exclude inversion.txt --range in tutorial\n",
    "    .filter(!(\n",
    "        ($\"contigName\".cast(\"int\") === 6 && $\"start\".between(25500000, 33500000)) ||\n",
    "        ($\"contigName\".cast(\"int\") === 8 && $\"start\".between(8135000, 12000000)) ||\n",
    "        ($\"contigName\".cast(\"int\") === 17 && $\"start\".between(40900000, 45000000)) \n",
    "    ))\n",
    "\n",
    "    // The raw data contains \"I\" and \"D\" alleles for insertion and deletion which will cause an error\n",
    "    // using the Glow vcf writer because the samtools java lib used under the hood doesn't understand\n",
    "    // these encodings.  For this operation sepcifically, the nature of the mutation doesn't matter\n",
    "    // so they can be recoded as if they were for SNPs instead of indels.  The results don't come \n",
    "    // out drastically different w/o this --> 104134 variants are kept instead of the expected 104144\n",
    "    .withColumn(\"referenceAllele\", when($\"referenceAllele\".isin(\"I\", \"D\"), lit(\"A\"))\n",
    "                .otherwise($\"referenceAllele\"))\n",
    "    .withColumn(\"alternateAlleles\", when($\"alternateAlleles\"(0).isin(\"I\", \"D\"), array(lit(\"T\")))\n",
    "                .otherwise($\"alternateAlleles\"))\n",
    "\n",
    "    // The LD prune algorithm in plink technically requires that all variants be present in the same run\n",
    "    // which means the same result won't be produced if the computation is done in parallel on separate\n",
    "    // partitions as if it is done on a single partition.  The variants removed probably change very little\n",
    "    // if this is done concurrently, but since we're going for equality with the plink-only results, use 1 partition\n",
    "    .repartition(1)\n",
    "\n",
    "    // Rows in vcf must be sorted by chromosome or plink with throw the error\n",
    "    // \".bim file has a split chromosome\"\n",
    "    .sortWithinPartitions($\"contigName\".cast(\"int\").asc, $\"start\".asc, $\"end\".asc)\n",
    "    \n",
    "    // Write the plink data as VCF \n",
    "    // WARNING: This will drop all sample phenotype/demographic data which plink will then treat as missing\n",
    "    //          so it is important to avoid operations that require this info\n",
    "    // NOTE: plink-pipe = \n",
    "    // #!/bin/bash\n",
    "    // prefix=plink-$(cat /proc/sys/kernel/random/uuid)\n",
    "    // plink --vcf /dev/stdin \"${@:2}\" --out prefix > /dev/null\n",
    "    // cat prefix.$1\n",
    "    .fn(d => \n",
    "        Glow.transform(\"pipe\", d, Map(\n",
    "            // prune.in = variants to keep, prune.out = variants to remove (they are single-col csvs)\n",
    "            \"cmd\" -> \"[\\\"plink-pipe\\\", \\\"prune.in\\\", \\\"--indep-pairwise\\\", \\\"50\\\", \\\"5\\\", \\\"0.2\\\"]\",\n",
    "            \"inputFormatter\" -> \"vcf\",\n",
    "            \"outputFormatter\" -> \"text\",\n",
    "            \"inVcfHeader\" -> \"infer\"\n",
    "        ))\n",
    "    )\n",
    "    .map(_.getAs[String](\"text\")).collect.toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:40:21.564447Z",
     "start_time": "2020-01-16T04:38:41.811Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 04:38:42 INFO PipeTransformer: hlsUsage:[pipe,{\"pipeCmdTool\":\"plink\"}]\n",
      "20/01/16 04:38:42 INFO VCFHeaderUtils$: Inferring header for VCF writer\n",
      "20/01/16 04:38:42 INFO Piper$: Beginning pipe with cmd List(plink-pipe, prune.in, --indep-pairwise, 50, 5, 0.2)\n",
      "20/01/16 04:39:36 INFO InternalRowToVariantContextConverter: Field StructField(position,DoubleType,true) is present in data schema but does not have a VCF representation\n",
      "20/01/16 04:39:36 INFO InternalRowToVariantContextConverter: Field StructField(isFounder,MapType(StringType,BooleanType,true),true) is present in data schema but does not have a VCF representation\n",
      "20/01/16 04:40:20 INFO VCFInputFormatter: Closing VCF input formatter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mct\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m104144\u001b[39m\n",
       "\u001b[36mres52_2\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m104144\u001b[39m"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// The plink tutorial results in the retention of 104,144 variants out of 1,457,897 total\n",
    "val ct = variants_to_keep.size\n",
    "assert(ct == 104144)\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:48:27.497456Z",
     "start_time": "2020-01-16T04:44:47.637Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 04:44:48 INFO PipeTransformer: hlsUsage:[pipe,{\"pipeCmdTool\":\"plink\"}]\n",
      "20/01/16 04:44:48 INFO VCFHeaderUtils$: Inferring header for VCF writer\n",
      "20/01/16 04:44:48 INFO Piper$: Beginning pipe with cmd List(plink-pipe, prune.in, --indep-pairwise, 50, 5, 0.2)\n",
      "20/01/16 04:45:40 INFO InternalRowToVariantContextConverter: Field StructField(position,DoubleType,true) is present in data schema but does not have a VCF representation\n",
      "20/01/16 04:45:40 INFO InternalRowToVariantContextConverter: Field StructField(isFounder,MapType(StringType,BooleanType,true),true) is present in data schema but does not have a VCF representation\n",
      "20/01/16 04:46:22 INFO VCFInputFormatter: Closing VCF input formatter\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "ok",
         "type": "bar",
         "x": [
          0.34700000000000003,
          0.34900000000000003,
          0.35000000000000003,
          0.35100000000000003,
          0.352,
          0.353,
          0.354,
          0.355,
          0.356,
          0.357,
          0.358,
          0.359,
          0.36,
          0.361,
          0.362
         ],
         "y": [
          1,
          4,
          7,
          15,
          18,
          21,
          29,
          30,
          17,
          5,
          6,
          4,
          2,
          3,
          1
         ]
        },
        {
         "name": "outlier",
         "type": "bar",
         "x": [
          0.339,
          0.343
         ],
         "y": [
          1,
          1
         ]
        }
       ],
       "layout": {
        "title": "Sample Heterozygosity Rate Distribution",
        "xaxis": {
         "title": "Heterozygosity Rate"
        },
        "yaxis": {
         "title": "Num Samples"
        }
       }
      },
      "text/html": [
       "<div class=\"chart\" id=\"plot-57ea1b8c-6d5e-43eb-9b9c-8cc542e032e6\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[0.34700000000000003,0.34900000000000003,0.35000000000000003,0.35100000000000003,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362],\"name\":\"ok\",\"y\":[1.0,4.0,7.0,15.0,18.0,21.0,29.0,30.0,17.0,5.0,6.0,4.0,2.0,3.0,1.0],\"type\":\"bar\"};\n",
       "  var data1 = {\"x\":[0.339,0.343],\"name\":\"outlier\",\"y\":[1.0,1.0],\"type\":\"bar\"};\n",
       "\n",
       "  var data = [data0, data1];\n",
       "  var layout = {\"title\":\"Sample Heterozygosity Rate Distribution\",\"yaxis\":{\"title\":\"Num Samples\"},\"xaxis\":{\"title\":\"Heterozygosity Rate\"}};\n",
       "\n",
       "  Plotly.plot('plot-57ea1b8c-6d5e-43eb-9b9c-8cc542e032e6', data, layout);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdf_het_rate\u001b[39m\n",
       "\u001b[36mres53_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-57ea1b8c-6d5e-43eb-9b9c-8cc542e032e6\"\u001b[39m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compare this distribution to the \"Heterozygosity Distribution\" section in the companion analysis\n",
    "def df_het_rate = df\n",
    "    // Restrict to only variants in linkage equilibrium\n",
    "    .filter($\"names\"(0).isin(variants_to_keep:_*))\n",
    "    // Limit to autosomal chromosomes as in https://www.cog-genomics.org/plink/1.9/basic_stats#ibc (--het)\n",
    "    .filter($\"contigName\".cast(\"int\").between(1, 22))\n",
    "\n",
    "    // Compute stats across variants\n",
    "    .selectExpr(\"sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc\")\n",
    "    .withColumn(\"qc\", explode($\"qc\"))\n",
    "\n",
    "    // Determine rate and outliers\n",
    "    .withColumn(\"hetRate\", $\"qc.nHet\".cast(\"double\") / $\"qc.nCalled\".cast(\"double\"))\n",
    "\n",
    "    // Identify outliers\n",
    "    .transform(d => {\n",
    "        d.crossJoin(d.agg(mean($\"hetRate\").as(\"hetRateMean\"), stddev($\"hetRate\").as(\"hetRateStd\")))\n",
    "        .withColumn(\"hetRateLow\", $\"hetRateMean\" - lit(3) * $\"hetRateStd\")\n",
    "        .withColumn(\"hetRateHigh\", $\"hetRateMean\" + lit(3) * $\"hetRateStd\")\n",
    "        .withColumn(\"status\", when(\n",
    "            $\"hetRate\".between($\"hetRateLow\", $\"hetRateHigh\"), lit(\"ok\")\n",
    "        ).otherwise(lit(\"outlier\")))\n",
    "    })\n",
    "\n",
    "df_het_rate\n",
    "    .withColumn(\"bin\", bround($\"hetRate\"/.001)*.001)\n",
    "    .groupBy(\"status\", \"bin\").count.sort($\"status\".asc, $\"bin\".asc)\n",
    "    .fn(d => {\n",
    "        Seq(\"ok\", \"outlier\").map(s =>\n",
    "            Bar(\n",
    "                x=d.filter($\"status\" === s).map(_.getAs[Double](\"bin\")).collect.toList,\n",
    "                y=d.filter($\"status\" === s).map(_.getAs[Long](\"count\")).collect.toList,\n",
    "                name=s\n",
    "            )\n",
    "        ).plot(\n",
    "            title=\"Sample Heterozygosity Rate Distribution\",\n",
    "            yaxis=Axis(title=\"Num Samples\"),\n",
    "            xaxis=Axis(title=\"Heterozygosity Rate\")\n",
    "        )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:51:27.404255Z",
     "start_time": "2020-01-16T04:45:35.077Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 04:48:27 INFO PipeTransformer: hlsUsage:[pipe,{\"pipeCmdTool\":\"plink\"}]\n",
      "20/01/16 04:48:27 INFO VCFHeaderUtils$: Inferring header for VCF writer\n",
      "20/01/16 04:48:27 INFO Piper$: Beginning pipe with cmd List(plink-pipe, prune.in, --indep-pairwise, 50, 5, 0.2)\n",
      "20/01/16 04:49:20 INFO InternalRowToVariantContextConverter: Field StructField(position,DoubleType,true) is present in data schema but does not have a VCF representation\n",
      "20/01/16 04:49:20 INFO InternalRowToVariantContextConverter: Field StructField(isFounder,MapType(StringType,BooleanType,true),true) is present in data schema but does not have a VCF representation\n",
      "20/01/16 04:50:03 INFO VCFInputFormatter: Closing VCF input formatter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 42.2 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_qc5\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 7 more fields]\n",
       "\u001b[36mct\u001b[39m: (\u001b[32mLong\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m1073226L\u001b[39m, \u001b[32m163\u001b[39m)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_qc5 = df\n",
    "    .crossJoin(\n",
    "        df_het_rate\n",
    "        .filter($\"status\" === \"outlier\")\n",
    "        .agg(collect_set($\"qc.sampleId\").as(\"outliers\"))\n",
    "    )\n",
    "    .withColumn(\"genotypes\", expr(\"filter(genotypes, g -> !array_contains(outliers, g.sampleId))\"))\n",
    "\n",
    "val ct = timeop(\"qc5\") {\n",
    "    count(df_qc5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:51:28.400371Z",
     "start_time": "2020-01-16T04:45:36.238Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(count(ss.read.format(\"plink\").load(data_dir / QC5_FILE + \".bed\" toString)) == ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T04:52:17.407927Z",
     "start_time": "2020-01-16T04:45:36.845Z"
    }
   },
   "outputs": [],
   "source": [
    "df_qc5.write.mode(\"overwrite\").parquet(data_dir / QC5_FILE + \".parquet\" toString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
